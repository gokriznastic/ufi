{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pgm(filename, byteorder='>'):\n",
    "    \"\"\"Return image data from a raw PGM file as numpy array.\n",
    "\n",
    "    Format specification: http://netpbm.sourceforge.net/doc/pgm.html\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        buffer = f.read()\n",
    "    try:\n",
    "        header, width, height, maxval = re.search( \n",
    "            b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\" \n",
    "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\" \n",
    "            b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\" \n",
    "            b\"(\\d+)\\s)\", buffer).groups()\n",
    "\n",
    "    except AttributeError:\n",
    "        raise ValueError(\"Not a raw PGM file: '%s'\" % filename)\n",
    "    return np.frombuffer(buffer,\n",
    "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
    "                            count=int(width)*int(height),\n",
    "                            offset=len(header)\n",
    "                            ).reshape((int(height), int(width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'ufi-cropped\\\\train'\n",
    "folders = [f for f in listdir(my_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    files.append([f for f in listdir(folder_path) if f.endswith('.pgm')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4316"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(files[i]) for i in range(len(folders)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_list = []\n",
    "train_labels = []\n",
    "for fo in range(len(folders)):\n",
    "    for fi in files[fo]:\n",
    "        pathname_list.append(join(my_path, join(folders[fo], fi)))\n",
    "        train_labels.append(folders[fo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4316"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pathname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.empty((4316, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image = read_pgm(pathname_list[0])\n",
    "train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pathname_list)):\n",
    "    train_images[i] = read_pgm(pathname_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (4316, 128, 128))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_images), train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_labels)):\n",
    "    train_labels[i] = train_labels[i][1:]\n",
    "    train_labels[i] = int(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.from_numpy(train_labels).long() #-- model was throwing error during training so labels had to be type casted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([4316]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels), train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'ufi-cropped\\\\test'\n",
    "folders = [f for f in listdir(my_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    files.append([f for f in listdir(folder_path) if f.endswith('.pgm')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(files[i]) for i in range(len(folders)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_list = []\n",
    "test_labels = []\n",
    "for fo in range(len(folders)):\n",
    "    for fi in files[fo]:\n",
    "        pathname_list.append(join(my_path, join(folders[fo], fi)))\n",
    "        test_labels.append(folders[fo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pathname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.empty((605, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = read_pgm(pathname_list[0])\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pathname_list)):\n",
    "    test_images[i] = read_pgm(pathname_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (605, 128, 128))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_images), test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_labels)):\n",
    "    test_labels[i] = test_labels[i][1:]\n",
    "    test_labels[i] = int(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = torch.from_numpy(test_labels).long() #-- model was throwing error during training so labels had to be type casted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([605]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_labels), test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UFIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 4316\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        image = torch.from_numpy(img)\n",
    "        image = image.float()\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufi_train_data = UFIDataset(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=ufi_train_data,\n",
    "                                         batch_size=60,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufi_test_data = UFIDataset(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=ufi_test_data,\n",
    "                                         batch_size=60,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 128, 128]), torch.Size([60]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample batch check\n",
    "data_iter = iter(train_loader)\n",
    "X, Y = data_iter.next()\n",
    "\n",
    "X.size(), Y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(folders)\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional neural network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=2, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(32*32*64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size()[0],-1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/5], step[10/72], loss: 55.527794\n",
      "epoch[1/5], step[20/72], loss: 33.558113\n",
      "epoch[1/5], step[30/72], loss: 29.217281\n",
      "epoch[1/5], step[40/72], loss: 23.060751\n",
      "epoch[1/5], step[50/72], loss: 14.108567\n",
      "epoch[1/5], step[60/72], loss: 12.615983\n",
      "epoch[1/5], step[70/72], loss: 11.072942\n",
      "epoch[2/5], step[10/72], loss: 5.728499\n",
      "epoch[2/5], step[20/72], loss: 4.439501\n",
      "epoch[2/5], step[30/72], loss: 3.286589\n",
      "epoch[2/5], step[40/72], loss: 2.952354\n",
      "epoch[2/5], step[50/72], loss: 2.917035\n",
      "epoch[2/5], step[60/72], loss: 1.626036\n",
      "epoch[2/5], step[70/72], loss: 1.824749\n",
      "epoch[3/5], step[10/72], loss: 1.193151\n",
      "epoch[3/5], step[20/72], loss: 0.567181\n",
      "epoch[3/5], step[30/72], loss: 0.437095\n",
      "epoch[3/5], step[40/72], loss: 0.354293\n",
      "epoch[3/5], step[50/72], loss: 0.456174\n",
      "epoch[3/5], step[60/72], loss: 0.603761\n",
      "epoch[3/5], step[70/72], loss: 0.384565\n",
      "epoch[4/5], step[10/72], loss: 0.456261\n",
      "epoch[4/5], step[20/72], loss: 0.126042\n",
      "epoch[4/5], step[30/72], loss: 0.161340\n",
      "epoch[4/5], step[40/72], loss: 0.315516\n",
      "epoch[4/5], step[50/72], loss: 0.315232\n",
      "epoch[4/5], step[60/72], loss: 0.080102\n",
      "epoch[4/5], step[70/72], loss: 0.072813\n",
      "epoch[5/5], step[10/72], loss: 0.127538\n",
      "epoch[5/5], step[20/72], loss: 0.014660\n",
      "epoch[5/5], step[30/72], loss: 0.062004\n",
      "epoch[5/5], step[40/72], loss: 0.004337\n",
      "epoch[5/5], step[50/72], loss: 0.141724\n",
      "epoch[5/5], step[60/72], loss: 0.009295\n",
      "epoch[5/5], step[70/72], loss: 0.024575\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #labels = torch.autograd.Variable(labels.long(), requires_grad=False)\n",
    "        # move tensors to device\n",
    "        images = images.reshape((-1, 1, 128, 128)).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward pass\n",
    "        pred = model.forward(images)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if((i+1)%10 == 0):\n",
    "            print('epoch[{}/{}], step[{}/{}], loss: {:4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.reshape((-1, 1, 128, 128)).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        pred = model.forward(images)\n",
    "        \n",
    "        predictions = torch.argmax(pred.data, 1)\n",
    "        \n",
    "        total += labels.size()[0]\n",
    "        correct += (predictions == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-accuracy: 99.35125115848007\n"
     ]
    }
   ],
   "source": [
    "print('training-accuracy:', (correct/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in zip(test_images, test_labels): #-- note that we aren't able to test_loader here due to some errors\n",
    "        images = torch.from_numpy(images)\n",
    "        images = images.float()\n",
    "        images = images.reshape((-1, 1, 128, 128)).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        pred = model.forward(images)\n",
    "        \n",
    "        predictions = torch.argmax(pred.data, 1)\n",
    "        \n",
    "        correct += (predictions == labels).sum().item()\n",
    "    total = test_labels.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing-accuracy: 38.34710743801653\n"
     ]
    }
   ],
   "source": [
    "print('testing-accuracy:', (correct/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ConvNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save the model checkpoint\n",
    "torch.save(model, 'UFI-best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model params checkpoint\n",
    "torch.save(model.state_dict(), 'UFI-best-params.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
